---
layout: single
title: Reflexión del nuevo sistema de Apple CSAM
excerpt: "Apple ha anunciado que implementará un sistema CSAM Detection para prevenir la pornografía infantil. ¿Puede ser que este sistema pueda suponer una amenaza para los usuarios?>
date: 2021-09-01
classes: wide
header:
  teaser: /assets/images/art-writeup-apple/apple_logo.png
  teaser_home_page: true
  icon: /assets/images/article.webp
categories:
  - articles
  - personal
tags:
  - privacy
  - apple
---

![](/assets/images/art-writeup-apple/apple_logo.png)

Una noticia muy interesante para todos aquellos que usamos productos de Apple y de la que deberíamos ser conscientes, para que al menos, cuando actualicemos a iOS 15, sepamos las consecuencias que puede tener sobre nuestra privacidad. Tiene que ver con un nuevo sistema que Apple ha puesto en marcha llamado CSAM Detection para combatir la pornografía infantil. Este sistema, a través de automatismos (a fin de evitar violaciones claras de privacidad), revisará nuestras fotos (las que tengamos sincronizadas en iCloud aparentemente) en busca de contenido que coincida con contenido reportado por agencias como pornografía infantil. ¿Y qué sucede cuando hay algún positivo entre nuestras Fotos? Que la foto y el resultado del análisis se mandan a los servidores de Apple para su comprobación. Y es aquí donde puede estar la violación de privacidad clara, en que alguién de Apple tiene que visualizar esa foto para confirmar si se trata de un verdadero positivo, descubriendo así la información que contenga esa imagen sobre nosotros. Y, como reflexión personal,  qué clase de fotos pensaís que son las que pueden tener semenjanza con un verdadero positivo de pornografía infantil? Al menos a mi parecer, serán las fotos comprometidas, con bastante proporción de desnudez, lo que no augura nada bueno en caso de darse un falso positivo, habiendo vislumbrado a una persona totalmente desnuda o cualquier acto íntimo, sin que exista pornografía infantil de por medio. La solución puesta por Apple a este problema, ha sido fijar un umbral de 30 fotos antes de revisarlas todas. Es decir, hasta que no reciba 30 potenciales avisos, esas fotos no serán visualizadas por nadie y permanecerán cifradas, aunque hayan salido del dispositivo del usuario. ¿Es 30 un umbral significativo? En Apple dicen que sí, que hace la probabilidad de error muy reducida. Yo, al menos, con esta reflexión pretendo informar a potenciales usuarios, para que cada uno valore si le parece adecuado y proceda a actualizar sus productos o no.

Para que nos hagamos una idea de lo controvertido del modelo, el artículo también refleja como entidades de la talla de Electronic Frontier Foundation (EFF) ya han criticado este modelo practicamente desde su origen.

Si os resulta interesante más allá de esta reflexión y explicación personal que comparto, os recomiendo leeros la noticia.

`https://latam.kaspersky.com/blog/what-is-apple-csam-detection/22723/`
